x-restart: &restart
  restart: "unless-stopped"
x-restart-failure: &restart_failure
  restart: "on-failure: 5"
x-logging: &logging
  logging:
    driver: local
    options:
      max-size: ${DOCKERLOGGING_MAXSIZE}
x-security: &security
  security_opt:
    - no-new-privileges:true
# https://rocm.docs.amd.com/projects/install-on-linux/en/docs-6.2.0/how-to/docker.html#docker-compose
x-rocm: &rocm
  devices:
    - /dev/kfd
    - /dev/dri
  security_opt:
    - seccomp:unconfined
  group_add:
    - video
x-rocm-env: &rocm-env
  # rocminfo | grep gfx1151 should return non-empty result if the workaround below is active
  HSA_OVERRIDE_GFX_VERSION: 11.0.0 # forces RDNA 3.5 compatibility since the 8060S isn't officially supported.
  HIP_VISIBLE_DEVICES: 0
  # If you encounter CL_OUT_OF_RESOURCES errors:
  # GGML_ROCM_WORKSPACE:4096  # Increase memory allocation

services:
  deepseek:
    image: mymlcimage
    build:
      context: "MLC-LLM for ROCm Radeon 8060S"
    container_name: deepseek
    command: --model /models/deepseek-70b-q8-mlc --device vulkan
    <<: [ *restart_failure, *logging, *security, *rocm ]
    environment:
      <<: *rocm-env
    volumes:
      - ${DOCKERDIR}/deepseek-70b-q8-mlc:/models/deepseek-70b-q8-mlc
    ports:
      - "${DEEPSEEK_PORT}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: rocm
              capabilities: [gpu]
              count: 1
          memory: 75g
    networks:
      - deepseek

  llava:
    image: mymlcimage
    build:
      context: "MLC-LLM for ROCm Radeon 8060S"
    container_name: llava
    command: --model /models/llava-34b-q8-mlc --device rocm
    <<: [ *restart_failure, *logging, *security, *rocm ]
    environment:
      <<: *rocm-env
    volumes:
      - ${DOCKERDIR}/llava-34b-q8-mlc:/models/llava-34b-q8-mlc
    ports:
      - "${LLAVA_PORT}:8001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: rocm
              count: 1
          memory: 32g
    networks:
      - llava

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    <<: [ *restart_failure, *logging, *security ]
    ports:
      - "${OPEN_WEBUI_PORT}:8080"
    environment:
      - OLLAMA_API_BASE_URL=http://mlc-server:8000
    depends_on:
      - mlc-server
    networks:
      - open-webui

  # https://github.com/SYSTRAN/faster-whisper/issues/162
  #   -> https://github.com/OpenNMT/CTranslate2/issues/1072
  #     -> Dockerfile https://gist.github.com/muaiyadh/d99923375f5d35b5b08e8369705fa41a
  #       -> with patch https://github.com/OpenNMT/CTranslate2/issues/1072#issuecomment-2686736426
  # faster-whisper:

  # https://github.com/mendableai/firecrawl/

  # https://github.com/dzhng/deep-research
  deep-research:
    image: mydeepresearch
    build:
      context: https://github.com/dzhng/deep-research#main
    container_name: deep-research
    <<: [ *restart_failure, *logging, *security ]
    environment:
      # FIRECRAWL_KEY: "YOUR_KEY"
      # If you want to use your self-hosted Firecrawl, add the following below:
      FIRECRAWL_BASE_URL: "http://localhost:3002"
      FIRECRAWL_CONCURRENCY: "2"
      
      # OPENAI_KEY: "YOUR_KEY"
      CONTEXT_SIZE: "128000"
      
      # If you want to use other OpenAI compatible API, add the following below:
      OPENAI_ENDPOINT: "http://localhost:11434/v1"
      CUSTOM_MODEL: "llama3.1"
    volumes:
      - ${DOCKERDIR}/deep-research/:/app/
    tty: true
    stdin_open: true

networks: # subnets are defined to avoid overlaps
  default:
    ipam:
      config:
        - subnet: 10.201.1.0/24
          gateway: 10.201.1.1

  deepseek:
    ipam:
      config:
        - subnet: 10.201.2.0/24

  llava:
    ipam:
      config:
        - subnet: 10.201.3.0/24

  llava:
    ipam:
      config:
        - subnet: 10.201.3.0/24

  open-webui:
    ipam:
      config:
        - subnet: 10.201.4.0/24

  deep-research:
    ipam:
      config:
        - subnet: 10.201.5.0/24
